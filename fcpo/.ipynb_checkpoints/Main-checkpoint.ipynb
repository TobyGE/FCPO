{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import ArgumentParser\n",
    "# from gym import make\n",
    "# from gym.spaces import Box, Discrete\n",
    "# import gym\n",
    "# import gym_fairrec\n",
    "# import roboschool\n",
    "# from yaml import load\n",
    "# import yaml\n",
    "\n",
    "from models import build_diag_gauss_policy, build_mlp, build_multinomial_policy\n",
    "# from simulators import *\n",
    "from transforms import *\n",
    "from torch_utils import get_device\n",
    "from trpo import TRPO\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# from read_data import read_file, read_embeddings, Embeddings\n",
    "\n",
    "from data_util import read_file\n",
    "from environment import *\n",
    "# from env import *\n",
    "# from ddpg import *\n",
    "import torch\n",
    "import os\n",
    "import time\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of users: 6040, num of items: 3952\n",
      "Successfully create Training Env!\n"
     ]
    }
   ],
   "source": [
    "model_name = 'fairrec'\n",
    "data_name = 'ml-1m'\n",
    "data = read_file('./data/'+data_name+'/train_data.csv')\n",
    "item_embeddings = np.load('./data/'+data_name+'/item_embed.npy')\n",
    "user_embeddings = np.load('./data/'+data_name+'/user_embed.npy')\n",
    "\n",
    "\n",
    "nb_item = item_embeddings.shape[0]\n",
    "nb_user = user_embeddings.shape[0]\n",
    "print('num of users: %d, num of items: %d' %(nb_user, nb_item))\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "env_args = {}\n",
    "env_args['data'] = data\n",
    "env_args['nb_user'] = nb_user\n",
    "env_args['nb_item'] = nb_item\n",
    "env_args['item_embeddings'] = item_embeddings\n",
    "env_args['user_embeddings'] = user_embeddings\n",
    "env_args['device'] = device\n",
    "env_args['gamma'] = 0.95\n",
    "\n",
    "env = Environment(**env_args)\n",
    "print('Successfully create Training Env!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=500, out_features=64, bias=True)\n",
      "  (1): Tanh()\n",
      "  (2): Linear(in_features=64, out_features=100, bias=True)\n",
      "  (3): DiagGaussianLayer()\n",
      ")\n",
      "Sequential(\n",
      "  (0): Linear(in_features=501, out_features=64, bias=True)\n",
      "  (1): Tanh()\n",
      "  (2): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "history_length = 5 # N in article\n",
    "ra_length = 1 # K in article\n",
    "state_space_size = item_embeddings.shape[1] * history_length\n",
    "action_space_size = item_embeddings.shape[1] * ra_length\n",
    "\n",
    "\n",
    "vf_hidden_dims = [64]\n",
    "vf_args = (state_space_size + 1, vf_hidden_dims, 1)\n",
    "value_fun = build_mlp(*vf_args)\n",
    "\n",
    "policy_hidden_dims = [64]\n",
    "policy_args = (state_space_size, policy_hidden_dims, action_space_size)\n",
    "policy = build_diag_gauss_policy(*policy_args)\n",
    "\n",
    "\n",
    "policy.to(device)\n",
    "value_fun.to(device)\n",
    "print(policy)\n",
    "print(value_fun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import gym\n",
    "from gym.spaces import Box, Discrete\n",
    "import numpy as np\n",
    "import torch\n",
    "# from torch_utils import get_device\n",
    "\n",
    "\n",
    "class Simulator:\n",
    "    def __init__(self, env, policy, n_trajectories, trajectory_len, **env_args):\n",
    "        self.env = env\n",
    "        self.policy = policy\n",
    "\n",
    "\n",
    "class SinglePathSimulator(Simulator):\n",
    "    def __init__(self, env, policy, n_trajectories, trajectory_len, **env_args):\n",
    "        Simulator.__init__(self, env, policy, n_trajectories, trajectory_len, **env_args)\n",
    "        self.item_embeddings= env_args['item_embeddings']\n",
    "        self.trajectory_len = trajectory_len\n",
    "        self.n_trajectories = n_trajectories\n",
    "        self.nb_item = env_args['nb_item']\n",
    "        self.device = env_args['device']\n",
    "\n",
    "    def sample_trajectories(self):\n",
    "        self.policy.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            memory = np.asarray([defaultdict(list) for i in range(self.n_trajectories)])\n",
    "        #     done = [False] * n_trajectories\n",
    "\n",
    "            ra_length = 1\n",
    "#             len_trajectory = 10\n",
    "#             epsilon = 0.9\n",
    "            item_embeds = torch.from_numpy(self.item_embeddings).to(self.device).float()\n",
    "\n",
    "#             memory_states = []\n",
    "#             memory_actions = []\n",
    "#             memory_rewards = []\n",
    "#             memory_done = []\n",
    "\n",
    "            score = 0\n",
    "            states = self.env.reset()\n",
    "            recommended_item_onehot = torch.FloatTensor(self.n_trajectories, self.nb_item).zero_().to(device)\n",
    "            recommendations = []\n",
    "            for t in range(self.trajectory_len): \n",
    "                policy_input = torch.FloatTensor(states).to(self.device).view(self.n_trajectories, -1)\n",
    "                weight_dists = self.policy(policy_input)\n",
    "                w = weight_dists.sample()\n",
    "                item_weights = torch.mm(w.view(-1,item_embeds.shape[1]), item_embeds.transpose(0,1)).view(self.n_trajectories, ra_length, -1)\n",
    "                item_weights = torch.mul(item_weights.transpose(0,1), 1-recommended_item_onehot).reshape(states.shape[0],ra_length,-1)\n",
    "                item_idxes = torch.argmax(item_weights,dim=2)\n",
    "\n",
    "                recommendations.append(item_idxes)\n",
    "                recommended_item_onehot = recommended_item_onehot.scatter_(1, item_idxes, 1)\n",
    "\n",
    "                actions = item_embeds[item_idxes.cpu().detach()]\n",
    "                states_prime, rewards, info = self.env.step(actions, item_idxes)\n",
    "\n",
    "        #         states_prime, rewards, info = env.step(item_idxes)\n",
    "        #         memory_states.append(policy_input)\n",
    "        #         memory_actions.append(actions)\n",
    "        #         memory_rewards.append(rewards)\n",
    "        #         memory_done.append(done)\n",
    "\n",
    "                for i in range(len(memory)):\n",
    "                    trajectory = memory[i]\n",
    "                    trajectory['states'].append(policy_input[i].to(device).squeeze())\n",
    "                    trajectory['actions'].append(actions[i].to(device).squeeze())\n",
    "                    trajectory['rewards'].append(rewards[i].to(device).squeeze())\n",
    "\n",
    "\n",
    "                states = states_prime\n",
    "                score += torch.sum(info).detach().cpu()\n",
    "                \n",
    "            for trajectory in memory:\n",
    "                trajectory['done'] = True    \n",
    "            print(score/self.trajectory_len)\n",
    "            print(torch.cat(recommendations,1))\n",
    "            return memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trajectories = nb_user\n",
    "trajectory_len = 10\n",
    "simulator = SinglePathSimulator(env, policy, n_trajectories, trajectory_len, **env_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    trpo_args = config['trpo_args']\n",
    "except:\n",
    "    trpo_args = {}\n",
    "\n",
    "trpo = TRPO(policy, value_fun, simulator, model_name=model_name,\n",
    "            continue_from_file=False, **trpo_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3179, 2492,  954,  ..., 3223, 3947, 3543],\n",
      "        [1855, 2506, 2194,  ..., 2092, 1923, 3615],\n",
      "        [3568,  828,  717,  ..., 1421, 1034, 3798],\n",
      "        ...,\n",
      "        [2430, 1404, 2314,  ..., 3577,  140, 2040],\n",
      "        [3158, 1242, 3821,  ..., 1981,  321, 3871],\n",
      "        [2238, 1725, 3162,  ..., 3909,  582,  713]], device='cuda:0')\n",
      "[EPISODE]: 1\t[AVG. REWARD]: 4.4139\t [ELAPSED TIME]: 0:00:10\n",
      "tensor([[3408, 3744, 2771,  ..., 1940, 3793, 1204],\n",
      "        [3090, 3846, 3879,  ..., 2549,  777, 1168],\n",
      "        [3124, 2605,  212,  ..., 1742, 1942, 3752],\n",
      "        ...,\n",
      "        [ 677, 2148, 1326,  ...,  179, 2065, 2313],\n",
      "        [3265, 2274, 3816,  ..., 2816,   69, 1599],\n",
      "        [   5, 1720,  972,  ..., 1915,  923, 2790]], device='cuda:0')\n",
      "[EPISODE]: 2\t[AVG. REWARD]: 4.4997\t [ELAPSED TIME]: 0:00:21\n",
      "tensor([[3577, 1210, 1214,  ..., 1249,  827,  607],\n",
      "        [2382,  241,  717,  ..., 3942, 2312, 2458],\n",
      "        [ 319, 2711, 1250,  ..., 3625, 3452, 1428],\n",
      "        ...,\n",
      "        [2871, 1494, 3826,  ..., 3625,  531, 1445],\n",
      "        [3162, 1326, 2699,  ..., 3514, 3687,  464],\n",
      "        [2313, 1242, 3676,  ..., 2426, 3082,  161]], device='cuda:0')\n",
      "[EPISODE]: 3\t[AVG. REWARD]: 4.6631\t [ELAPSED TIME]: 0:00:32\n",
      "tensor([[1516, 1923, 2020,  ..., 1182, 3578, 1420],\n",
      "        [1430, 1832, 3863,  ..., 1923, 2235,  495],\n",
      "        [ 215, 2829, 3570,  ...,  296, 3177, 2492],\n",
      "        ...,\n",
      "        [3912, 2360, 3159,  ..., 3885, 3909, 2395],\n",
      "        [1693, 3007, 3420,  ..., 3539,  306, 2882],\n",
      "        [ 492, 3180, 3757,  ..., 1544, 2926, 1752]], device='cuda:0')\n",
      "[EPISODE]: 4\t[AVG. REWARD]: 4.7805\t [ELAPSED TIME]: 0:00:44\n",
      "tensor([[2359, 1553, 1688,  ..., 1217,  533, 2863],\n",
      "        [1793,  460, 3420,  ...,  406, 3364, 3888],\n",
      "        [1923, 3843, 2313,  ..., 1021, 3543, 2582],\n",
      "        ...,\n",
      "        [3784, 1003, 2237,  ..., 1049, 2524, 2878],\n",
      "        [ 601,  298, 3035,  ..., 1297, 2360, 3341],\n",
      "        [1058, 2157,  230,  ..., 3261, 2937, 2274]], device='cuda:0')\n",
      "[EPISODE]: 5\t[AVG. REWARD]: 4.9240\t [ELAPSED TIME]: 0:00:55\n",
      "tensor([[2570, 3519, 1444,  ..., 2066, 3052,  468],\n",
      "        [2711,  317, 2857,  ..., 3753, 1147, 2104],\n",
      "        [  16,   44, 2656,  ..., 3114, 1124,  287],\n",
      "        ...,\n",
      "        [1545, 1034,  505,  ..., 2360, 1079, 2238],\n",
      "        [ 677, 3670, 3675,  ..., 3073, 2426, 2429],\n",
      "        [3919, 3502,  212,  ..., 2942, 3193, 2966]], device='cuda:0')\n",
      "[EPISODE]: 6\t[AVG. REWARD]: 4.9992\t [ELAPSED TIME]: 0:01:07\n",
      "tensor([[3162, 3787, 2785,  ..., 3200, 2995, 2159],\n",
      "        [1844, 3224, 1725,  ...,  523, 2202, 1236],\n",
      "        [2524, 1182, 3323,  ...,  719,   83, 2172],\n",
      "        ...,\n",
      "        [2336,  890, 2709,  ..., 2235, 1747,   69],\n",
      "        [3628, 3750, 2135,  ..., 1958, 1205, 2924],\n",
      "        [3509, 1455, 3261,  ..., 2358, 2360,  351]], device='cuda:0')\n",
      "[EPISODE]: 7\t[AVG. REWARD]: 5.1228\t [ELAPSED TIME]: 0:01:19\n",
      "tensor([[1188, 1922,  734,  ..., 1191,  594, 3078],\n",
      "        [2274,  909, 2322,  ..., 2971, 2656, 3758],\n",
      "        [1205, 3327, 1865,  ..., 1899, 3455, 1209],\n",
      "        ...,\n",
      "        [1565, 2659, 2549,  ..., 2362,  278, 2948],\n",
      "        [1353, 2018, 3872,  ..., 3082, 2924, 3012],\n",
      "        [3909, 1672, 1220,  ..., 3675, 1599,  800]], device='cuda:0')\n",
      "[EPISODE]: 8\t[AVG. REWARD]: 5.2576\t [ELAPSED TIME]: 0:01:30\n",
      "tensor([[ 296, 2313, 3258,  ..., 1175,  289,  921],\n",
      "        [1896, 1217, 1968,  ..., 2699,  598,   68],\n",
      "        [ 828, 1150, 3939,  ..., 1281, 1415, 2561],\n",
      "        ...,\n",
      "        [3657, 1393, 2021,  ..., 1973, 2926,  259],\n",
      "        [3912, 2386, 2958,  ..., 1295,  607, 1896],\n",
      "        [3184, 1981, 2717,  ..., 3625,   41, 3181]], device='cuda:0')\n",
      "[EPISODE]: 9\t[AVG. REWARD]: 5.3243\t [ELAPSED TIME]: 0:01:42\n",
      "tensor([[1787, 2130,  607,  ..., 3085, 1278,  440],\n",
      "        [1545, 3339, 1182,  ..., 1865,   11, 3275],\n",
      "        [3159, 2646, 3237,  ..., 1217, 1033, 2383],\n",
      "        ...,\n",
      "        [2051, 1526, 3909,  ..., 3568, 3924,  319],\n",
      "        [2161, 3149, 3006,  ..., 3326, 1257, 2892],\n",
      "        [3657, 2205,  531,  ..., 2058,   96, 3175]], device='cuda:0')\n",
      "[EPISODE]: 10\t[AVG. REWARD]: 5.4108\t [ELAPSED TIME]: 0:01:54\n"
     ]
    }
   ],
   "source": [
    "n_episodes = 10\n",
    "trpo.train(n_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = trpo.simulator.sample_trajectories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6., device='cuda:0')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(samples[0]['rewards'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "mean() received an invalid combination of arguments - got (out=NoneType, axis=NoneType, dtype=NoneType, ), but expected one of:\n * (torch.dtype dtype)\n * (tuple of names dim, bool keepdim, torch.dtype dtype)\n      didn't match because some of the keywords were incorrect: out, axis\n * (tuple of ints dim, bool keepdim, torch.dtype dtype)\n      didn't match because some of the keywords were incorrect: out, axis\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-16a08bf60bba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmean_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrajectory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rewards'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtrajectory\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mmean\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mmean\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m   3330\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3331\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3332\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3334\u001b[0m     return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "\u001b[0;31mTypeError\u001b[0m: mean() received an invalid combination of arguments - got (out=NoneType, axis=NoneType, dtype=NoneType, ), but expected one of:\n * (torch.dtype dtype)\n * (tuple of names dim, bool keepdim, torch.dtype dtype)\n      didn't match because some of the keywords were incorrect: out, axis\n * (tuple of ints dim, bool keepdim, torch.dtype dtype)\n      didn't match because some of the keywords were incorrect: out, axis\n"
     ]
    }
   ],
   "source": [
    "mean_reward = np.mean(torch.stack([np.sum(trajectory['rewards']) for trajectory in samples]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'torch.dtype' object has no attribute 'type'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-1ef2c21fa6ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrajectory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rewards'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtrajectory\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mmean\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mmean\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m   3333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3334\u001b[0m     return _methods._mean(a, axis=axis, dtype=dtype,\n\u001b[0;32m-> 3335\u001b[0;31m                           out=out, **kwargs)\n\u001b[0m\u001b[1;32m   3336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.7/site-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_mean\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mrcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mrcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mret\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mrcount\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'torch.dtype' object has no attribute 'type'"
     ]
    }
   ],
   "source": [
    "np.mean([np.sum(trajectory['rewards']) for trajectory in samples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
