{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import ArgumentParser\n",
    "# from gym import make\n",
    "# from gym.spaces import Box, Discrete\n",
    "# import gym\n",
    "# import gym_fairrec\n",
    "# import roboschool\n",
    "# from yaml import load\n",
    "# import yaml\n",
    "\n",
    "from models import build_diag_gauss_policy, build_mlp, build_multinomial_policy\n",
    "# from simulators import *\n",
    "from transforms import *\n",
    "from torch_utils import get_device\n",
    "from trpo import TRPO\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# from read_data import read_file, read_embeddings, Embeddings\n",
    "\n",
    "from data_util import read_file\n",
    "from fair_env import *\n",
    "# from env import *\n",
    "# from ddpg import *\n",
    "import torch\n",
    "import os\n",
    "import time\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of users: 6040, num of items: 3952\n",
      "Successfully create Training Env!\n"
     ]
    }
   ],
   "source": [
    "model_name = 'fairrec'\n",
    "data_name = 'ml-1m'\n",
    "data = read_file('./data/'+data_name+'/train_data.csv')\n",
    "item_embeddings = np.load('./data/'+data_name+'/item_embed.npy')\n",
    "user_embeddings = np.load('./data/'+data_name+'/user_embed.npy')\n",
    "item_indicator = np.load('./data/'+data_name+'/item_indicator_28.npy')\n",
    "\n",
    "\n",
    "nb_item = item_embeddings.shape[0]\n",
    "nb_user = user_embeddings.shape[0]\n",
    "print('num of users: %d, num of items: %d' %(nb_user, nb_item))\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "env_args = {}\n",
    "env_args['data'] = data\n",
    "env_args['nb_user'] = nb_user\n",
    "env_args['nb_item'] = nb_item\n",
    "env_args['item_embeddings'] = item_embeddings\n",
    "env_args['user_embeddings'] = user_embeddings\n",
    "env_args['item_indicator'] = item_indicator\n",
    "env_args['device'] = device\n",
    "env_args['gamma'] = 0.95\n",
    "\n",
    "env = Environment(**env_args)\n",
    "print('Successfully create Training Env!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=500, out_features=64, bias=True)\n",
      "  (1): Tanh()\n",
      "  (2): Linear(in_features=64, out_features=100, bias=True)\n",
      "  (3): DiagGaussianLayer()\n",
      ")\n",
      "Sequential(\n",
      "  (0): Linear(in_features=501, out_features=64, bias=True)\n",
      "  (1): Tanh()\n",
      "  (2): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "history_length = 5 # N in article\n",
    "ra_length = 1 # K in article\n",
    "state_space_size = item_embeddings.shape[1] * history_length\n",
    "action_space_size = item_embeddings.shape[1] * ra_length\n",
    "\n",
    "\n",
    "vf_hidden_dims = [64]\n",
    "vf_args = (state_space_size + 1, vf_hidden_dims, 1)\n",
    "value_fun = build_mlp(*vf_args)\n",
    "\n",
    "policy_hidden_dims = [64]\n",
    "policy_args = (state_space_size, policy_hidden_dims, action_space_size)\n",
    "policy = build_diag_gauss_policy(*policy_args)\n",
    "\n",
    "\n",
    "policy.to(device)\n",
    "value_fun.to(device)\n",
    "print(policy)\n",
    "print(value_fun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import gym\n",
    "from gym.spaces import Box, Discrete\n",
    "import numpy as np\n",
    "import torch\n",
    "# from torch_utils import get_device\n",
    "\n",
    "\n",
    "class Simulator:\n",
    "    def __init__(self, env, policy, n_trajectories, trajectory_len, **env_args):\n",
    "        self.env = env\n",
    "        self.policy = policy\n",
    "\n",
    "\n",
    "class SinglePathSimulator(Simulator):\n",
    "    def __init__(self, env, policy, n_trajectories, trajectory_len, **env_args):\n",
    "        Simulator.__init__(self, env, policy, n_trajectories, trajectory_len, **env_args)\n",
    "        self.item_embeddings= env_args['item_embeddings']\n",
    "        self.trajectory_len = trajectory_len\n",
    "        self.n_trajectories = n_trajectories\n",
    "        self.nb_item = env_args['nb_item']\n",
    "        self.device = env_args['device']\n",
    "\n",
    "    def sample_trajectories(self):\n",
    "        self.policy.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            memory = np.asarray([defaultdict(list) for i in range(self.n_trajectories)])\n",
    "        #     done = [False] * n_trajectories\n",
    "\n",
    "            ra_length = 1\n",
    "#             len_trajectory = 10\n",
    "#             epsilon = 0.9\n",
    "            item_embeds = torch.from_numpy(self.item_embeddings).to(self.device).float()\n",
    "\n",
    "#             memory_states = []\n",
    "#             memory_actions = []\n",
    "#             memory_rewards = []\n",
    "#             memory_done = []\n",
    "\n",
    "            ave_score = 0\n",
    "            ave_cost = 0\n",
    "            states = self.env.reset()\n",
    "            recommended_item_onehot = torch.FloatTensor(self.n_trajectories, self.nb_item).zero_().to(device)\n",
    "            recommendations = []\n",
    "            for t in range(self.trajectory_len): \n",
    "                policy_input = torch.FloatTensor(states).to(self.device).view(self.n_trajectories, -1)\n",
    "                weight_dists = self.policy(policy_input)\n",
    "                w = weight_dists.sample()\n",
    "                item_weights = torch.mm(w.view(-1,item_embeds.shape[1]), item_embeds.transpose(0,1)).view(self.n_trajectories, ra_length, -1)\n",
    "                item_weights = torch.mul(item_weights.transpose(0,1), 1-recommended_item_onehot).reshape(states.shape[0],ra_length,-1)\n",
    "                item_idxes = torch.argmax(item_weights,dim=2)\n",
    "\n",
    "                recommendations.append(item_idxes)\n",
    "                recommended_item_onehot = recommended_item_onehot.scatter_(1, item_idxes, 1)\n",
    "\n",
    "                actions = item_embeds[item_idxes.cpu().detach()]\n",
    "                states_prime, rewards, costs, info = self.env.step(actions, item_idxes)\n",
    "\n",
    "        #         states_prime, rewards, info = env.step(item_idxes)\n",
    "        #         memory_states.append(policy_input)\n",
    "        #         memory_actions.append(actions)\n",
    "        #         memory_rewards.append(rewards)\n",
    "        #         memory_done.append(done)\n",
    "\n",
    "                for i in range(len(memory)):\n",
    "                    trajectory = memory[i]\n",
    "                    trajectory['states'].append(policy_input[i].to(device).squeeze())\n",
    "                    trajectory['actions'].append(actions[i].to(device).squeeze())\n",
    "                    trajectory['rewards'].append(rewards[i].to(device).squeeze())\n",
    "\n",
    "\n",
    "                states = states_prime\n",
    "                ave_score += torch.sum(info).detach().cpu()\n",
    "                ave_cost += torch.sum(costs).detach().cpu()\n",
    "            for trajectory in memory:\n",
    "                trajectory['done'] = True    \n",
    "            print(ave_score/self.trajectory_len, ave_cost/self.trajectory_len)\n",
    "            print(torch.cat(recommendations,1))\n",
    "            return memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trajectories = nb_user\n",
    "trajectory_len = 10\n",
    "simulator = SinglePathSimulator(env, policy, n_trajectories, trajectory_len, **env_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    trpo_args = config['trpo_args']\n",
    "except:\n",
    "    trpo_args = {}\n",
    "\n",
    "trpo = TRPO(policy, value_fun, simulator, model_name=model_name,\n",
    "            continue_from_file=False, **trpo_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1890) tensor(1030.3000)\n",
      "tensor([[2382,  662, 2654,  ..., 2852, 2853, 3820],\n",
      "        [1492, 3645,  809,  ..., 1706, 1896, 2494],\n",
      "        [3862, 1273, 2482,  ..., 2605, 3184, 2695],\n",
      "        ...,\n",
      "        [3162,  448, 3659,  ..., 2505, 3554, 1333],\n",
      "        [3027,  117, 2582,  ...,  233, 1720,  875],\n",
      "        [3689, 1957, 1088,  ..., 1240, 2559, 1349]], device='cuda:0')\n",
      "[EPISODE]: 1\t[AVG. REWARD]: 3.1293\t [ELAPSED TIME]: 0:00:10\n",
      "tensor(1969) tensor(1050.1000)\n",
      "tensor([[2300,  817, 2313,  ..., 2330, 1335, 3819],\n",
      "        [1494, 3867, 2360,  ..., 2458, 1923, 1957],\n",
      "        [2494, 1940, 1600,  ..., 3264, 2116, 1393],\n",
      "        ...,\n",
      "        [ 212, 2642,  174,  ..., 3326, 2492,  122],\n",
      "        [1983,  601,   85,  ..., 3216, 2548,  444],\n",
      "        [ 239, 2426,   61,  ...,  321,  215, 2317]], device='cuda:0')\n",
      "[EPISODE]: 2\t[AVG. REWARD]: 3.2608\t [ELAPSED TIME]: 0:00:21\n",
      "tensor(2044) tensor(1088.3000)\n",
      "tensor([[3527, 3158,  464,  ..., 3862, 1688, 3006],\n",
      "        [2336,  122, 3445,  ..., 3073,  713, 1231],\n",
      "        [1720, 1813, 2115,  ..., 2362, 1996, 3618],\n",
      "        ...,\n",
      "        [ 392,  428,  802,  ..., 3325, 1720, 3116],\n",
      "        [2549, 3565, 2971,  ..., 3911, 3548, 1353],\n",
      "        [1273,  104, 1516,  ..., 1274, 1033,  213]], device='cuda:0')\n",
      "[EPISODE]: 3\t[AVG. REWARD]: 3.3843\t [ELAPSED TIME]: 0:00:32\n",
      "tensor(2122) tensor(1145.3000)\n",
      "tensor([[1329, 1584, 3675,  ..., 1894,  417, 1923],\n",
      "        [2613,  610, 2458,  ..., 3863, 3398, 3145],\n",
      "        [3020,   72, 1553,  ..., 3003, 3912, 3445],\n",
      "        ...,\n",
      "        [2734, 2158, 3091,  ..., 2768, 3946, 2163],\n",
      "        [1593,   27,  230,  ...,  746, 2037, 2313],\n",
      "        [ 630, 3224, 3403,  ..., 2274, 2881, 2680]], device='cuda:0')\n",
      "[EPISODE]: 4\t[AVG. REWARD]: 3.5137\t [ELAPSED TIME]: 0:00:43\n",
      "tensor(2182) tensor(1156.1000)\n",
      "tensor([[1968, 2825,  495,  ...,  337, 1923, 2435],\n",
      "        [1354, 3626, 1586,  ...,  809,  734, 1983],\n",
      "        [ 326,  407,  746,  ..., 3224,  817, 2360],\n",
      "        ...,\n",
      "        [2153,  198, 2775,  ..., 2711, 3697,  204],\n",
      "        [2305,   72, 2605,  ..., 3519, 1923, 3692],\n",
      "        [2324, 3863,   24,  ..., 2371, 1011, 2699]], device='cuda:0')\n",
      "[EPISODE]: 5\t[AVG. REWARD]: 3.6136\t [ELAPSED TIME]: 0:00:55\n",
      "tensor(2248) tensor(1199.7000)\n",
      "tensor([[2790, 2711, 2458,  ..., 3604, 2644,  761],\n",
      "        [ 416, 1281, 3876,  ..., 3027, 1286, 1240],\n",
      "        [3604, 3625, 3687,  ..., 1488, 2150, 1706],\n",
      "        ...,\n",
      "        [3596, 1455, 1545,  ..., 2381, 2231, 3719],\n",
      "        [3319,  215, 3141,  ..., 1347, 3237, 3020],\n",
      "        [1204, 3783, 1310,  ..., 3274, 2633, 3456]], device='cuda:0')\n",
      "[EPISODE]: 6\t[AVG. REWARD]: 3.7225\t [ELAPSED TIME]: 0:01:06\n",
      "tensor(2329) tensor(1229.1000)\n",
      "tensor([[1725, 3820, 3006,  ..., 3469, 2162, 1626],\n",
      "        [3944, 2360, 2435,  ..., 1961, 1011, 2107],\n",
      "        [1936, 2383, 2693,  ..., 2735,  533, 1255],\n",
      "        ...,\n",
      "        [1033,   81, 3539,  ..., 1260,  931, 1479],\n",
      "        [2709, 2768, 1193,  ..., 1534,  828, 1210],\n",
      "        [ 106, 1412, 1273,  ...,  105, 2062, 3882]], device='cuda:0')\n",
      "[EPISODE]: 7\t[AVG. REWARD]: 3.8575\t [ELAPSED TIME]: 0:01:17\n",
      "tensor(2421) tensor(1265.1000)\n",
      "tensor([[3029, 3158, 2759,  ...,   24,  319,  666],\n",
      "        [2773, 2059, 1984,  ..., 1175, 3851, 3326],\n",
      "        [2593, 1883, 2511,  ..., 2863, 1095, 1347],\n",
      "        ...,\n",
      "        [ 406, 2328,  764,  ...,  105, 3816, 2511],\n",
      "        [2158, 1231, 3914,  ...,  460, 2481,  846],\n",
      "        [ 254, 3445, 3027,  ..., 1833, 3181, 3768]], device='cuda:0')\n",
      "[EPISODE]: 8\t[AVG. REWARD]: 4.0096\t [ELAPSED TIME]: 0:01:29\n",
      "tensor(2502) tensor(1307.2000)\n",
      "tensor([[2237, 1175, 3751,  ..., 3944, 2294,  617],\n",
      "        [3784, 2065, 2092,  ...,  702,  972, 3618],\n",
      "        [3284, 1172, 1210,  ..., 2960, 3224, 3076],\n",
      "        ...,\n",
      "        [2519, 1427, 3570,  ...,  189, 3285, 3090],\n",
      "        [2799, 3705, 3298,  ..., 3912, 1175, 2353],\n",
      "        [2960, 1599, 2617,  ..., 3717, 2395, 1029]], device='cuda:0')\n",
      "[EPISODE]: 9\t[AVG. REWARD]: 4.1439\t [ELAPSED TIME]: 0:01:40\n",
      "tensor(2582) tensor(1357.1000)\n",
      "tensor([[2182, 3675, 3411,  ...,  445,   96,  719],\n",
      "        [2857, 1463, 3693,  ..., 3744, 3503, 3570],\n",
      "        [2623, 1844, 1182,  ...,  612, 3445, 3895],\n",
      "        ...,\n",
      "        [3326, 2892, 2323,  ..., 1310, 3746,  287],\n",
      "        [3124, 3909,  261,  ..., 3337, 1899,  717],\n",
      "        [1982, 1455, 3393,  ..., 3099, 3625, 1278]], device='cuda:0')\n",
      "[EPISODE]: 10\t[AVG. REWARD]: 4.2753\t [ELAPSED TIME]: 0:01:52\n"
     ]
    }
   ],
   "source": [
    "n_episodes = 10\n",
    "trpo.train(n_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
