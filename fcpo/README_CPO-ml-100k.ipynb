{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import ArgumentParser\n",
    "from cpo import CPO\n",
    "from memory import Memory\n",
    "from models import Actor, Critic\n",
    "# from simulators import SinglePathSimulator\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from data_util import read_file\n",
    "# from fair_env_simulator import *\n",
    "from fair_env import *\n",
    "import torch\n",
    "import os\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "from gini import gini\n",
    "import matplotlib.pyplot as plt\n",
    "from test_ranking import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of users: 943, num of items: 1682\n",
      "Successfully create Training Env!\n"
     ]
    }
   ],
   "source": [
    "model_name = 'cpo'\n",
    "data_name = 'ml-100k'\n",
    "data = read_file('../data/'+data_name+'/train_data.csv')\n",
    "item_embeddings = np.load('../data/'+data_name+'/pmf_item_embed.npy')\n",
    "user_embeddings = np.load('../data/'+data_name+'/pmf_user_embed.npy')\n",
    "item_indicator = np.load('../data/'+data_name+'/item_cost_indicator_28.npy')\n",
    "\n",
    "nb_item = item_embeddings.shape[0]\n",
    "nb_user = user_embeddings.shape[0]\n",
    "print('num of users: %d, num of items: %d' %(nb_user, nb_item))\n",
    "\n",
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "env_args = {}\n",
    "env_args['data'] = data\n",
    "env_args['nb_user'] = nb_user\n",
    "env_args['nb_item'] = nb_item\n",
    "env_args['item_embeddings'] = item_embeddings\n",
    "env_args['user_embeddings'] = user_embeddings\n",
    "env_args['item_indicator'] = item_indicator\n",
    "env_args['device'] = device\n",
    "env_args['gamma'] = 0.95\n",
    "env_args['frac'] = 1\n",
    "\n",
    "env = Environment(**env_args)\n",
    "print('Successfully create Training Env!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, namedtuple\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from autoassign import autoassign\n",
    "from memory import Memory, Trajectory\n",
    "from torch_utils.torch_utils import get_device\n",
    "from gini import gini\n",
    "import random\n",
    "\n",
    "\n",
    "class Simulator:\n",
    "    def __init__(self, env, policy, n_trajectories, trajectory_len, **env_args):\n",
    "        self.env = env\n",
    "        self.policy = policy\n",
    "\n",
    "\n",
    "class SinglePathSimulator(Simulator):\n",
    "    def __init__(self, env, policy, n_trajectories, trajectory_len, **env_args):\n",
    "        Simulator.__init__(self, env, policy, n_trajectories, trajectory_len, **env_args)\n",
    "        self.item_embeddings= env_args['item_embeddings']\n",
    "        self.trajectory_len = trajectory_len\n",
    "        self.n_trajectories = n_trajectories\n",
    "        self.nb_item = env_args['nb_item']\n",
    "        self.device = env_args['device']\n",
    "        self.hit_rate = []\n",
    "        self.gini_coefficient = []\n",
    "        self.pop_rate = []\n",
    "\n",
    "    def run_sim(self):\n",
    "        self.policy.eval()\n",
    "        with torch.no_grad():\n",
    "            trajectories = np.asarray([Trajectory() for i in range(self.n_trajectories)])\n",
    "            ra_length = 1\n",
    "            \n",
    "            epsilon_start = 0.2\n",
    "            epsilon_end = 0.1\n",
    "            \n",
    "            item_embeds = torch.from_numpy(self.item_embeddings).to(self.device).float()\n",
    "\n",
    "            ave_score = 0\n",
    "            ave_cost = 0\n",
    "            states = self.env.reset()\n",
    "            users = self.env.current_user\n",
    "            \n",
    "            recommended_item_onehot = torch.FloatTensor(self.n_trajectories, self.nb_item).zero_().to(self.device)\n",
    "            recommendations = []\n",
    "            for t in range(self.trajectory_len): \n",
    "                \"\"\"\n",
    "                epsilon decay\n",
    "                \"\"\"\n",
    "                temp_r = max((self.trajectory_len-t)/self.trajectory_len, 0)\n",
    "                epsilon = (epsilon_start - epsilon_end) * temp_r + epsilon_end\n",
    "                \n",
    "                policy_input = torch.FloatTensor(states).to(self.device)\n",
    "                user_input = torch.FloatTensor(users).to(self.device)\n",
    "\n",
    "                if np.random.rand() >= epsilon:\n",
    "                    weight_dists = self.policy(policy_input, user_input)\n",
    "                    w = weight_dists.sample()\n",
    "#                     print(w.shape, item_embeds.shape)\n",
    "#                     input()\n",
    "#                     w.view(-1,item_embeds.shape[1])\n",
    "#                     w = weight_dists.mean\n",
    "                    item_weights = torch.mm(w, item_embeds.transpose(0,1)).view(self.n_trajectories, ra_length, -1)\n",
    "#                     print(item_weights.shape)\n",
    "#                     input()\n",
    "                    item_weights = torch.mul(item_weights.transpose(0,1), 1-recommended_item_onehot).reshape(states.shape[0],ra_length,-1)\n",
    "                    item_idxes = torch.argmax(item_weights,dim=2)\n",
    "                else:\n",
    "                    item_weights = torch.FloatTensor(self.n_trajectories, ra_length, nb_item).uniform_(0, 1).to(device)\n",
    "                    item_weights = torch.mul(item_weights.transpose(0,1), 1-recommended_item_onehot).reshape(states.shape[0],ra_length,-1)\n",
    "                    item_idxes = torch.argmax(item_weights,dim=2)\n",
    "\n",
    "                recommendations.append(item_idxes)\n",
    "                recommended_item_onehot = recommended_item_onehot.scatter_(1, item_idxes, 1)\n",
    "\n",
    "                actions = item_embeds[item_idxes.cpu().detach()]\n",
    "                states_prime, users_prime, rewards, costs, info = self.env.step(actions, item_idxes)\n",
    "\n",
    "                for i in range(len(trajectories)):\n",
    "#                     if rewards[i] == 0 and random.random() <= 0.5:\n",
    "#                         continue\n",
    "#                     else:\n",
    "                    trajectory = trajectories[i]\n",
    "                    trajectory.observations.append(policy_input[i].to(self.device).squeeze())\n",
    "                    trajectory.users.append(user_input[i].to(self.device).squeeze())\n",
    "                    trajectory.actions.append(actions[i].to(self.device).squeeze())\n",
    "                    trajectory.rewards.append(rewards[i].to(self.device).squeeze())\n",
    "                    trajectory.costs.append(costs[i].to(self.device).squeeze())\n",
    "\n",
    "\n",
    "                states = states_prime\n",
    "                users = users_prime\n",
    "                ave_score += torch.sum(info).detach().cpu()\n",
    "                ave_cost += torch.sum(costs).detach().cpu()\n",
    "                 \n",
    "            memory = Memory(trajectories)\n",
    "    \n",
    "#             print(ave_score.float()/(self.trajectory_len*self.n_trajectories), ave_cost/(self.trajectory_len*self.n_trajectories))\n",
    "#             self.pop_rate.append(ave_cost/(self.trajectory_len*self.n_trajectories))\n",
    "\n",
    "#             recommendation_tensor = torch.cat(recommendations,1)\n",
    "#             idx, val = torch.unique(torch.cat(recommendations), return_counts=True)\n",
    "#             hr = (ave_score.float()/(self.trajectory_len*self.n_trajectories)).cpu().numpy()\n",
    "#             self.hit_rate.append(hr)\n",
    "            \n",
    "#             val_ = torch.cat((val.float(),torch.zeros(self.nb_item-len(val)).to(self.device)))\n",
    "#             g = gini(val_.cpu().numpy())\n",
    "#             self.gini_coefficient.append(g)\n",
    "            \n",
    "            return memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step Len.: tensor(0.9000, device='cuda:3')\n",
      "[Episode]: 1 | [Avg. Reward]: 0.8822905421257019 | [Avg. Cost]: 3.7476139068603516 | [Elapsed Time]: 0:00:08\n",
      "Step Len.: tensor(0.9000, device='cuda:3')\n",
      "[Episode]: 2 | [Avg. Reward]: 0.7433722019195557 | [Avg. Cost]: 3.121951103210449 | [Elapsed Time]: 0:00:17\n",
      "Step Len.: tensor(0.9000, device='cuda:3')\n",
      "[Episode]: 3 | [Avg. Reward]: 0.924708366394043 | [Avg. Cost]: 4.029692649841309 | [Elapsed Time]: 0:00:26\n",
      "Step Len.: tensor(1.0000, device='cuda:3')\n",
      "[Episode]: 4 | [Avg. Reward]: 0.8388122916221619 | [Avg. Cost]: 3.7794272899627686 | [Elapsed Time]: 0:00:35\n",
      "Step Len.: tensor(1.0000, device='cuda:3')\n",
      "[Episode]: 5 | [Avg. Reward]: 0.7338281869888306 | [Avg. Cost]: 3.3541886806488037 | [Elapsed Time]: 0:00:45\n",
      "Step Len.: tensor(1.0000, device='cuda:3')\n",
      "[Episode]: 6 | [Avg. Reward]: 0.7804877758026123 | [Avg. Cost]: 3.420996904373169 | [Elapsed Time]: 0:00:54\n",
      "Step Len.: tensor(1.0000, device='cuda:3')\n",
      "[Episode]: 7 | [Avg. Reward]: 0.9003181457519531 | [Avg. Cost]: 4.0180277824401855 | [Elapsed Time]: 0:01:03\n",
      "Step Len.: tensor(1.0000, device='cuda:3')\n",
      "[Episode]: 8 | [Avg. Reward]: 0.9194061756134033 | [Avg. Cost]: 4.055143356323242 | [Elapsed Time]: 0:01:13\n",
      "Step Len.: tensor(1.0000, device='cuda:3')\n",
      "[Episode]: 9 | [Avg. Reward]: 0.738070011138916 | [Avg. Cost]: 3.119830369949341 | [Elapsed Time]: 0:01:22\n",
      "Step Len.: tensor(1.0000, device='cuda:3')\n",
      "[Episode]: 10 | [Avg. Reward]: 0.8345705270767212 | [Avg. Cost]: 3.611876964569092 | [Elapsed Time]: 0:01:31\n",
      "Step Len.: tensor(1.0000, device='cuda:3')\n",
      "[Episode]: 11 | [Avg. Reward]: 0.9045599102973938 | [Avg. Cost]: 4.015906810760498 | [Elapsed Time]: 0:01:40\n",
      "Step Len.: tensor(1.0000, device='cuda:3')\n",
      "[Episode]: 12 | [Avg. Reward]: 0.8515376448631287 | [Avg. Cost]: 3.6415693759918213 | [Elapsed Time]: 0:01:50\n",
      "Step Len.: tensor(1.0000, device='cuda:3')\n",
      "[Episode]: 13 | [Avg. Reward]: 0.7518557906150818 | [Avg. Cost]: 3.3319194316864014 | [Elapsed Time]: 0:01:59\n",
      "Step Len.: tensor(1.0000, device='cuda:3')\n",
      "[Episode]: 14 | [Avg. Reward]: 0.8748674392700195 | [Avg. Cost]: 4.028632164001465 | [Elapsed Time]: 0:02:08\n",
      "Step Len.: tensor(1.0000, device='cuda:3')\n",
      "[Episode]: 15 | [Avg. Reward]: 0.7041357159614563 | [Avg. Cost]: 3.17391300201416 | [Elapsed Time]: 0:02:17\n",
      "Step Len.: tensor(1.0000, device='cuda:3')\n",
      "[Episode]: 16 | [Avg. Reward]: 0.8229056000709534 | [Avg. Cost]: 3.7391304969787598 | [Elapsed Time]: 0:02:27\n",
      "Step Len.: tensor(1.0000, device='cuda:3')\n",
      "[Episode]: 17 | [Avg. Reward]: 0.7571579813957214 | [Avg. Cost]: 3.282078504562378 | [Elapsed Time]: 0:02:36\n",
      "Step Len.: tensor(1.0000, device='cuda:3')\n",
      "[Episode]: 18 | [Avg. Reward]: 0.7613998055458069 | [Avg. Cost]: 3.420996904373169 | [Elapsed Time]: 0:02:46\n",
      "Step Len.: tensor(1.0000, device='cuda:3')\n",
      "[Episode]: 19 | [Avg. Reward]: 0.8515376448631287 | [Avg. Cost]: 3.6012725830078125 | [Elapsed Time]: 0:02:55\n",
      "Step Len.: tensor(0.9000, device='cuda:3')\n",
      "[Episode]: 20 | [Avg. Reward]: 0.7910922765731812 | [Avg. Cost]: 3.5047719478607178 | [Elapsed Time]: 0:03:04\n",
      "Step Len.: tensor(1.0000, device='cuda:3')\n",
      "[Episode]: 21 | [Avg. Reward]: 0.890774130821228 | [Avg. Cost]: 4.025450706481934 | [Elapsed Time]: 0:03:13\n",
      "Step Len.: tensor(1.0000, device='cuda:3')\n",
      "[Episode]: 22 | [Avg. Reward]: 0.8812301158905029 | [Avg. Cost]: 3.881230115890503 | [Elapsed Time]: 0:03:22\n",
      "Step Len.: tensor(1.0000, device='cuda:3')\n",
      "[Episode]: 23 | [Avg. Reward]: 0.9734888672828674 | [Avg. Cost]: 4.060445308685303 | [Elapsed Time]: 0:03:31\n",
      "Step Len.: tensor(1.0000, device='cuda:3')\n",
      "[Episode]: 24 | [Avg. Reward]: 0.8812301158905029 | [Avg. Cost]: 3.8939554691314697 | [Elapsed Time]: 0:03:40\n",
      "Step Len.: tensor(1.0000, device='cuda:3')\n",
      "[Episode]: 25 | [Avg. Reward]: 0.8610816597938538 | [Avg. Cost]: 3.9024391174316406 | [Elapsed Time]: 0:03:49\n",
      "Step Len.: tensor(1.0001, device='cuda:3')\n",
      "[Episode]: 26 | [Avg. Reward]: 0.8292682766914368 | [Avg. Cost]: 3.6447508335113525 | [Elapsed Time]: 0:03:59\n",
      "Step Len.: tensor(0.9000, device='cuda:3')\n",
      "[Episode]: 27 | [Avg. Reward]: 0.8239660859107971 | [Avg. Cost]: 3.5662777423858643 | [Elapsed Time]: 0:04:08\n",
      "Step Len.: tensor(0.8998, device='cuda:3')\n",
      "[Episode]: 28 | [Avg. Reward]: 0.7942736148834229 | [Avg. Cost]: 3.544008493423462 | [Elapsed Time]: 0:04:17\n",
      "Step Len.: tensor(0.9997, device='cuda:3')\n",
      "[Episode]: 29 | [Avg. Reward]: 0.7751855850219727 | [Avg. Cost]: 3.4708378314971924 | [Elapsed Time]: 0:04:26\n",
      "Step Len.: tensor(1.0001, device='cuda:3')\n",
      "[Episode]: 30 | [Avg. Reward]: 0.7571579813957214 | [Avg. Cost]: 3.337221622467041 | [Elapsed Time]: 0:04:35\n",
      "Step Len.: tensor(0.9996, device='cuda:3')\n",
      "[Episode]: 31 | [Avg. Reward]: 0.9034994840621948 | [Avg. Cost]: 4.1325554847717285 | [Elapsed Time]: 0:04:45\n",
      "Step Len.: tensor(1.0008, device='cuda:3')\n",
      "[Episode]: 32 | [Avg. Reward]: 0.8557794094085693 | [Avg. Cost]: 3.686108112335205 | [Elapsed Time]: 0:04:54\n",
      "Step Len.: tensor(0.9001, device='cuda:3')\n",
      "[Episode]: 33 | [Avg. Reward]: 0.9268292784690857 | [Avg. Cost]: 3.9575822353363037 | [Elapsed Time]: 0:05:03\n",
      "Step Len.: tensor(0.8998, device='cuda:3')\n",
      "[Episode]: 34 | [Avg. Reward]: 0.7592788934707642 | [Avg. Cost]: 3.249204635620117 | [Elapsed Time]: 0:05:14\n",
      "Step Len.: tensor(0.8998, device='cuda:3')\n",
      "[Episode]: 35 | [Avg. Reward]: 0.8271474242210388 | [Avg. Cost]: 3.7560975551605225 | [Elapsed Time]: 0:05:23\n",
      "Step Len.: tensor(0.9999, device='cuda:3')\n",
      "[Episode]: 36 | [Avg. Reward]: 0.783669114112854 | [Avg. Cost]: 3.3902440071105957 | [Elapsed Time]: 0:05:32\n",
      "Step Len.: tensor(0.9998, device='cuda:3')\n",
      "[Episode]: 37 | [Avg. Reward]: 0.9978790879249573 | [Avg. Cost]: 4.139978885650635 | [Elapsed Time]: 0:05:42\n",
      "Step Len.: tensor(1.0000, device='cuda:3')\n",
      "[Episode]: 38 | [Avg. Reward]: 0.8992576599121094 | [Avg. Cost]: 3.944856882095337 | [Elapsed Time]: 0:05:51\n",
      "Step Len.: tensor(1.0000, device='cuda:3')\n",
      "[Episode]: 39 | [Avg. Reward]: 0.8939554691314697 | [Avg. Cost]: 3.7529163360595703 | [Elapsed Time]: 0:06:01\n",
      "Step Len.: tensor(0.9000, device='cuda:3')\n",
      "[Episode]: 40 | [Avg. Reward]: 0.8812301158905029 | [Avg. Cost]: 3.6606574058532715 | [Elapsed Time]: 0:06:10\n",
      "Step Len.: tensor(0.9001, device='cuda:3')\n",
      "[Episode]: 41 | [Avg. Reward]: 0.858960747718811 | [Avg. Cost]: 3.9194061756134033 | [Elapsed Time]: 0:06:20\n",
      "Step Len.: tensor(0.8997, device='cuda:3')\n",
      "[Episode]: 42 | [Avg. Reward]: 0.9734888672828674 | [Avg. Cost]: 4.130434989929199 | [Elapsed Time]: 0:06:29\n",
      "Step Len.: tensor(0.8999, device='cuda:3')\n",
      "[Episode]: 43 | [Avg. Reward]: 0.9342523813247681 | [Avg. Cost]: 3.955461263656616 | [Elapsed Time]: 0:06:39\n",
      "Step Len.: tensor(1.0004, device='cuda:3')\n",
      "[Episode]: 44 | [Avg. Reward]: 0.8398727178573608 | [Avg. Cost]: 3.960763454437256 | [Elapsed Time]: 0:06:49\n",
      "Step Len.: tensor(0.9993, device='cuda:3')\n",
      "[Episode]: 45 | [Avg. Reward]: 0.7773064970970154 | [Avg. Cost]: 3.4422056674957275 | [Elapsed Time]: 0:07:00\n",
      "Step Len.: tensor(0.8994, device='cuda:3')\n",
      "[Episode]: 46 | [Avg. Reward]: 0.8716861009597778 | [Avg. Cost]: 3.7158005237579346 | [Elapsed Time]: 0:07:12\n",
      "Step Len.: tensor(1.0012, device='cuda:3')\n",
      "[Episode]: 47 | [Avg. Reward]: 0.9851537942886353 | [Avg. Cost]: 4.249204635620117 | [Elapsed Time]: 0:07:21\n",
      "Step Len.: tensor(1.0005, device='cuda:3')\n",
      "[Episode]: 48 | [Avg. Reward]: 0.8971368074417114 | [Avg. Cost]: 3.6288440227508545 | [Elapsed Time]: 0:07:30\n",
      "Step Len.: tensor(0.8994, device='cuda:3')\n",
      "[Episode]: 49 | [Avg. Reward]: 0.8006362915039062 | [Avg. Cost]: 3.527041435241699 | [Elapsed Time]: 0:07:39\n",
      "Step Len.: tensor(1.0012, device='cuda:3')\n",
      "[Episode]: 50 | [Avg. Reward]: 0.9066808223724365 | [Avg. Cost]: 3.9257688522338867 | [Elapsed Time]: 0:07:49\n",
      "Step Len.: tensor(0.8981, device='cuda:3')\n",
      "[Episode]: 51 | [Avg. Reward]: 0.9830328822135925 | [Avg. Cost]: 4.158006191253662 | [Elapsed Time]: 0:07:58\n",
      "Step Len.: tensor(0.9006, device='cuda:3')\n",
      "[Episode]: 52 | [Avg. Reward]: 0.8260869383811951 | [Avg. Cost]: 3.7370095252990723 | [Elapsed Time]: 0:08:07\n",
      "Step Len.: tensor(0.9983, device='cuda:3')\n",
      "[Episode]: 53 | [Avg. Reward]: 0.9257688522338867 | [Avg. Cost]: 4.1410393714904785 | [Elapsed Time]: 0:08:16\n",
      "Step Len.: tensor(0.8977, device='cuda:3')\n",
      "[Episode]: 54 | [Avg. Reward]: 0.923647940158844 | [Avg. Cost]: 4.027571678161621 | [Elapsed Time]: 0:08:26\n",
      "Step Len.: tensor(0.9961, device='cuda:3')\n",
      "[Episode]: 55 | [Avg. Reward]: 0.7794273495674133 | [Avg. Cost]: 3.626723289489746 | [Elapsed Time]: 0:08:36\n",
      "Step Len.: tensor(0.9020, device='cuda:3')\n",
      "[Episode]: 56 | [Avg. Reward]: 0.9703075289726257 | [Avg. Cost]: 4.300106048583984 | [Elapsed Time]: 0:08:47\n",
      "Step Len.: tensor(0.8993, device='cuda:3')\n",
      "[Episode]: 57 | [Avg. Reward]: 0.9374337196350098 | [Avg. Cost]: 4.174973487854004 | [Elapsed Time]: 0:08:57\n",
      "Step Len.: tensor(0.9058, device='cuda:3')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-a32cd013c15c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0mmodelPath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"model/\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mdata_name\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/tempmodel_\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m \u001b[0mcpo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_episodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodelPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/FCPO/cpo(user_gru)/cpo.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, n_episodes, save_path, is_train, test_memory)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m             self.update_nn_regressor(self.value_fun, self.value_optimizer, states_w_time_train,\n\u001b[0;32m--> 149\u001b[0;31m                                      disc_rewards_train, self.val_l2_reg, self.val_iters)\n\u001b[0m\u001b[1;32m    150\u001b[0m             self.update_nn_regressor(self.cost_fun, self.cost_optimizer, states_w_time_train,\n\u001b[1;32m    151\u001b[0m                                      disc_costs_train, self.cost_l2_reg, self.cost_iters)\n",
      "\u001b[0;32m~/FCPO/cpo(user_gru)/cpo.py\u001b[0m in \u001b[0;36mupdate_nn_regressor\u001b[0;34m(self, nn_regressor, optimizer, states, targets, l2_reg_coef, n_iters)\u001b[0m\n\u001b[1;32m    293\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcalc_dual_vars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/optim/lbfgs.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    435\u001b[0m                     \u001b[0;31m# no use to re-evaluate that function here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 437\u001b[0;31m                         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    438\u001b[0m                     \u001b[0mflat_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gather_flat_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m                     \u001b[0mopt_cond\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflat_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mtolerance_grad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/FCPO/cpo(user_gru)/cpo.py\u001b[0m in \u001b[0;36mmse\u001b[0;34m()\u001b[0m\n\u001b[1;32m    289\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0ml2_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history_length = 5 \n",
    "ra_length = 1 \n",
    "item_embedding_size = item_embeddings.shape[1]\n",
    "user_embedding_size = user_embeddings.shape[1]\n",
    "\n",
    "\n",
    "vf_hidden_dims = [64]\n",
    "vf_args = (history_length, ra_length, user_embedding_size+item_embedding_size+item_embedding_size+1, vf_hidden_dims, 1)\n",
    "value_fun = Critic(*vf_args)\n",
    "\n",
    "cost_fun = Critic(*vf_args)\n",
    "\n",
    "policy_hidden_dims = [64]\n",
    "policy_args = (history_length, ra_length, user_embedding_size+item_embedding_size+item_embedding_size, policy_hidden_dims, item_embedding_size)\n",
    "policy = Actor(*policy_args)\n",
    "\n",
    "policy.to(device)\n",
    "value_fun.to(device)\n",
    "cost_fun.to(device)\n",
    "# print(policy)\n",
    "# print(value_fun)\n",
    "# print(cost_fun)\n",
    "\n",
    "n_trajectories = env.nb_user\n",
    "trajectory_len = 10\n",
    "simulator = SinglePathSimulator(env, policy, n_trajectories, trajectory_len, **env_args)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "max_val_step: lr for value_fun\n",
    "max_cost_step: lr for cost_fun\n",
    "line_search_max_step_len: initial search step for policy\n",
    "line_search_coef: search step decay rate\n",
    "\"\"\"\n",
    "max_constraint_val = 10\n",
    "cpo = CPO(policy, value_fun, cost_fun, simulator, device, model_name=model_name, \\\n",
    "          max_kl=1e-4, max_val_step=5e-2, max_cost_step=5e-2, max_constraint_val=max_constraint_val, \\\n",
    "          val_l2_reg=1e-3, cost_l2_reg=1e-3, discount_val=0.99, discount_cost=0.99, \\\n",
    "          line_search_max_step_len=1, line_search_coef=0.9, line_search_max_iter=20, \\\n",
    "          line_search_accept_ratio=1e-2, continue_from_file=False)\n",
    "\n",
    "n_episodes = 100\n",
    "\n",
    "modelPath = \"model/\"+data_name+\"/tempmodel_\" + data_name + '_'\n",
    "cpo.train(n_episodes, modelPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fair_env import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from data_util import read_file\n",
    "# from fair_env_simulator import *\n",
    "# from fair_env import *\n",
    "import torch\n",
    "import os\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "from gini import gini\n",
    "import matplotlib.pyplot as plt\n",
    "from test_ranking import *\n",
    "\n",
    "\"\"\"\n",
    "10:_\n",
    "4:_1.0413574\n",
    "3:_0.8780488\n",
    "\"\"\"\n",
    "data_name = 'ml-100k'\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "modelPath = \"model/\"+data_name+\"/tempmodel_\" + data_name + '_1.0413574.pkl'\n",
    "model = torch.load(modelPath)\n",
    "model.to(device)\n",
    "item_embeddings = np.load('../data/'+data_name+'/pmf_item_embed.npy')\n",
    "user_embeddings = np.load('../data/'+data_name+'/pmf_user_embed.npy')\n",
    "item_indicator = np.load('../data/'+data_name+'/item_cost_indicator_28.npy')\n",
    "\n",
    "nb_item = item_embeddings.shape[0]\n",
    "nb_user = user_embeddings.shape[0]\n",
    "\n",
    "history_length = 5 # N in article\n",
    "ra_length = 1 # K in article\n",
    "\n",
    "with torch.no_grad():  \n",
    "    test_data = read_file('../data/'+data_name+'/test_data.csv')\n",
    "    test_env_args = {}\n",
    "    test_env_args['data'] = test_data\n",
    "    test_env_args['nb_user'] = nb_user\n",
    "    test_env_args['nb_item'] = nb_item\n",
    "    test_env_args['item_embeddings'] = item_embeddings\n",
    "    test_env_args['user_embeddings'] = user_embeddings\n",
    "    test_env_args['item_indicator'] = item_indicator\n",
    "    test_env_args['device'] = device\n",
    "    test_env_args['gamma'] = 0.95\n",
    "    test_env_args['frac'] = 1\n",
    "    \n",
    "    test_trajectory_len = 200\n",
    "    test_env = Environment(**test_env_args)\n",
    "    states = test_env.reset()\n",
    "    users = test_env.current_user\n",
    "    item_embeds = torch.from_numpy(item_embeddings).to(device).float()\n",
    "    \n",
    "    num_click = 0\n",
    "    num_cost = 0\n",
    "    test_res = []\n",
    "    recommendations = []\n",
    "    recommended_item_onehot = torch.FloatTensor(test_env.nb_user, test_env.nb_item).zero_().to(device)  \n",
    "    test_gini_coefficient = []\n",
    "    test_pop_rate = []\n",
    "    \n",
    "    for t in range(test_trajectory_len):\n",
    "        policy_input = torch.FloatTensor(states).to(device)\n",
    "        user_input = torch.FloatTensor(users).to(device)\n",
    "        weight_dists = model(policy_input, user_input)\n",
    "#         w = weight_dists.sample()\n",
    "        w = weight_dists.mean\n",
    "        item_weights = torch.mm(w.view(-1,item_embeds.shape[1]), item_embeds.transpose(0,1)).view(test_env.nb_user, ra_length, -1)\n",
    "        item_weights = torch.mul(item_weights.transpose(0,1), 1-recommended_item_onehot).reshape(states.shape[0],ra_length,-1)\n",
    "        item_idxes = torch.argmax(item_weights,dim=2)\n",
    "        actions = item_embeds[item_idxes.cpu().detach()]\n",
    "        recommendations.append(item_idxes.squeeze())\n",
    "        recommended_item_onehot = recommended_item_onehot.scatter_(1, item_idxes, 1)\n",
    "        \n",
    "        states_prime, user_prime, rewards, costs, test_info = test_env.step(actions, item_idxes)\n",
    "        states = states_prime\n",
    "\n",
    "        num_click += torch.sum(test_info)\n",
    "        num_cost += torch.sum(costs).detach().cpu()\n",
    "        test_pop_rate.append(num_cost/((t+1)*states.shape[0]))\n",
    "        idx, val = torch.unique(torch.stack(recommendations), return_counts=True)\n",
    "        \n",
    "        test_res.append(test_info.squeeze())\n",
    "        val_ = torch.cat((val.float(),torch.zeros(nb_item-len(val)).to(device)))\n",
    "        g = gini(val_.cpu().numpy())\n",
    "        test_gini_coefficient.append(g)\n",
    "        \n",
    "    recommendation_indicator = item_indicator[torch.stack(recommendations).transpose(0,1).detach().cpu().numpy()]\n",
    "    pop_rate = []\n",
    "    for i in range(recommendation_indicator.shape[1]):\n",
    "        nb_rec = (i+1)*recommendation_indicator.shape[0]\n",
    "        nb_pop = np.sum(recommendation_indicator[:, :i+1])\n",
    "        pop_rate.append(nb_pop/nb_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.stack(recommendations).transpose(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_k = 200\n",
    "user_history_length = torch.sum(test_env.current_user_history, 1).detach().cpu().numpy()\n",
    "test_res_ = torch.stack(test_res).transpose(0,1).detach().cpu().numpy()\n",
    "report = get_test_results(test_res_, user_history_length, test_gini_coefficient, max_k)\n",
    "\n",
    "%matplotlib inline\n",
    "recall = np.mean(report[\"recall\"],0)\n",
    "hit_rate = np.mean(report[\"hit_rate\"],0)\n",
    "precision = np.mean(report[\"precision\"],0)\n",
    "ndcg = np.mean(report[\"ndcg\"],0)\n",
    "gini_index = report[\"gini_index\"]\n",
    "\n",
    "plot_k = 100\n",
    "plt.figure(figsize = (10,10))\n",
    "plt.subplot(3,2,1)\n",
    "plt.plot(np.arange(plot_k), recall[:plot_k])\n",
    "plt.title(\"Recall\")\n",
    "plt.subplot(3,2,2)\n",
    "plt.plot(np.arange(plot_k), hit_rate[:plot_k])\n",
    "plt.title(\"Hit Rate\")\n",
    "plt.subplot(3,2,3)\n",
    "plt.plot(np.arange(plot_k), precision[:plot_k])\n",
    "plt.title(\"Precision\")\n",
    "plt.subplot(3,2,4)\n",
    "plt.plot(np.arange(plot_k), ndcg[:plot_k])\n",
    "plt.title(\"NDCG\")\n",
    "plt.subplot(3,2,5)\n",
    "plt.plot(np.arange(plot_k), gini_index[:plot_k])\n",
    "plt.title(\"Gini\")\n",
    "plt.subplot(3,2,6)\n",
    "plt.plot(np.arange(plot_k), pop_rate[:plot_k])\n",
    "plt.title(\"Pop Rate\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_constraint_val = 4\n",
    "np.save('../results/'+data_name+'/'+model_name+'_'+str(max_constraint_val)+'_recall.npy', recall)\n",
    "np.save('../results/'+data_name+'/'+model_name+'_'+str(max_constraint_val)+'_hit_rate.npy', hit_rate)\n",
    "np.save('../results/'+data_name+'/'+model_name+'_'+str(max_constraint_val)+'_precision.npy', precision)\n",
    "np.save('../results/'+data_name+'/'+model_name+'_'+str(max_constraint_val)+'_ndcg.npy', ndcg)\n",
    "np.save('../results/'+data_name+'/'+model_name+'_'+str(max_constraint_val)+'_gini_index.npy', gini_index)\n",
    "np.save('../results/'+data_name+'/'+model_name+'_'+str(max_constraint_val)+'_pop_rate.npy', pop_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndcg[99], gini_index[99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndcg[99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndcg[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
