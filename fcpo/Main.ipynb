{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import ArgumentParser\n",
    "# from gym import make\n",
    "# from gym.spaces import Box, Discrete\n",
    "# import gym\n",
    "# import gym_fairrec\n",
    "# import roboschool\n",
    "# from yaml import load\n",
    "# import yaml\n",
    "\n",
    "from models import build_diag_gauss_policy, build_mlp, build_multinomial_policy\n",
    "# from simulators import *\n",
    "from transforms import *\n",
    "from torch_utils import get_device\n",
    "from trpo import TRPO\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# from read_data import read_file, read_embeddings, Embeddings\n",
    "\n",
    "from data_util import read_file\n",
    "from environment import *\n",
    "# from env import *\n",
    "# from ddpg import *\n",
    "import torch\n",
    "import os\n",
    "import time\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of users: 6040, num of items: 3952\n",
      "Successfully create Training Env!\n"
     ]
    }
   ],
   "source": [
    "model_name = 'fairrec'\n",
    "data_name = 'ml-1m'\n",
    "data = read_file('./data/'+data_name+'/train_data.csv')\n",
    "item_embeddings = np.load('./data/'+data_name+'/item_embed.npy')\n",
    "user_embeddings = np.load('./data/'+data_name+'/user_embed.npy')\n",
    "\n",
    "\n",
    "nb_item = item_embeddings.shape[0]\n",
    "nb_user = user_embeddings.shape[0]\n",
    "print('num of users: %d, num of items: %d' %(nb_user, nb_item))\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "env_args = {}\n",
    "env_args['data'] = data\n",
    "env_args['nb_user'] = nb_user\n",
    "env_args['nb_item'] = nb_item\n",
    "env_args['item_embeddings'] = item_embeddings\n",
    "env_args['user_embeddings'] = user_embeddings\n",
    "env_args['device'] = device\n",
    "env_args['gamma'] = 0.95\n",
    "\n",
    "env = Environment(**env_args)\n",
    "print('Successfully create Training Env!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=500, out_features=64, bias=True)\n",
      "  (1): Tanh()\n",
      "  (2): Linear(in_features=64, out_features=100, bias=True)\n",
      "  (3): DiagGaussianLayer()\n",
      ")\n",
      "Sequential(\n",
      "  (0): Linear(in_features=501, out_features=64, bias=True)\n",
      "  (1): Tanh()\n",
      "  (2): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "history_length = 5 # N in article\n",
    "ra_length = 1 # K in article\n",
    "state_space_size = item_embeddings.shape[1] * history_length\n",
    "action_space_size = item_embeddings.shape[1] * ra_length\n",
    "\n",
    "\n",
    "vf_hidden_dims = [64]\n",
    "vf_args = (state_space_size + 1, vf_hidden_dims, 1)\n",
    "value_fun = build_mlp(*vf_args)\n",
    "\n",
    "policy_hidden_dims = [64]\n",
    "policy_args = (state_space_size, policy_hidden_dims, action_space_size)\n",
    "policy = build_diag_gauss_policy(*policy_args)\n",
    "\n",
    "\n",
    "policy.to(device)\n",
    "value_fun.to(device)\n",
    "print(policy)\n",
    "print(value_fun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import gym\n",
    "from gym.spaces import Box, Discrete\n",
    "import numpy as np\n",
    "import torch\n",
    "# from torch_utils import get_device\n",
    "\n",
    "\n",
    "class Simulator:\n",
    "    def __init__(self, env, policy, n_trajectories, trajectory_len, **env_args):\n",
    "        self.env = env\n",
    "        self.policy = policy\n",
    "\n",
    "\n",
    "class SinglePathSimulator(Simulator):\n",
    "    def __init__(self, env, policy, n_trajectories, trajectory_len, **env_args):\n",
    "        Simulator.__init__(self, env, policy, n_trajectories, trajectory_len, **env_args)\n",
    "        self.item_embeddings= env_args['item_embeddings']\n",
    "        self.trajectory_len = trajectory_len\n",
    "        self.n_trajectories = n_trajectories\n",
    "        self.nb_item = env_args['nb_item']\n",
    "        self.device = env_args['device']\n",
    "\n",
    "    def sample_trajectories(self):\n",
    "        self.policy.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            memory = np.asarray([defaultdict(list) for i in range(self.n_trajectories)])\n",
    "        #     done = [False] * n_trajectories\n",
    "\n",
    "            ra_length = 1\n",
    "#             len_trajectory = 10\n",
    "#             epsilon = 0.9\n",
    "            item_embeds = torch.from_numpy(self.item_embeddings).to(self.device).float()\n",
    "\n",
    "#             memory_states = []\n",
    "#             memory_actions = []\n",
    "#             memory_rewards = []\n",
    "#             memory_done = []\n",
    "\n",
    "            score = 0\n",
    "            states = self.env.reset()\n",
    "            recommended_item_onehot = torch.FloatTensor(self.n_trajectories, self.nb_item).zero_().to(device)\n",
    "            recommendations = []\n",
    "            for t in range(self.trajectory_len): \n",
    "                policy_input = torch.FloatTensor(states).to(self.device).view(self.n_trajectories, -1)\n",
    "                weight_dists = self.policy(policy_input)\n",
    "                w = weight_dists.sample()\n",
    "                item_weights = torch.mm(w.view(-1,item_embeds.shape[1]), item_embeds.transpose(0,1)).view(self.n_trajectories, ra_length, -1)\n",
    "                item_weights = torch.mul(item_weights.transpose(0,1), 1-recommended_item_onehot).reshape(states.shape[0],ra_length,-1)\n",
    "                item_idxes = torch.argmax(item_weights,dim=2)\n",
    "\n",
    "                recommendations.append(item_idxes)\n",
    "                recommended_item_onehot = recommended_item_onehot.scatter_(1, item_idxes, 1)\n",
    "\n",
    "                actions = item_embeds[item_idxes.cpu().detach()]\n",
    "                states_prime, rewards, info = self.env.step(actions, item_idxes)\n",
    "\n",
    "        #         states_prime, rewards, info = env.step(item_idxes)\n",
    "        #         memory_states.append(policy_input)\n",
    "        #         memory_actions.append(actions)\n",
    "        #         memory_rewards.append(rewards)\n",
    "        #         memory_done.append(done)\n",
    "\n",
    "                for i in range(len(memory)):\n",
    "                    trajectory = memory[i]\n",
    "                    trajectory['states'].append(policy_input[i].to(device).squeeze())\n",
    "                    trajectory['actions'].append(actions[i].to(device).squeeze())\n",
    "                    trajectory['rewards'].append(rewards[i].to(device).squeeze())\n",
    "\n",
    "\n",
    "                states = states_prime\n",
    "                score += torch.sum(info).detach().cpu()\n",
    "                \n",
    "            for trajectory in memory:\n",
    "                trajectory['done'] = True    \n",
    "            print(score/self.trajectory_len)\n",
    "            print(torch.cat(recommendations,1))\n",
    "            return memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trajectories = nb_user\n",
    "trajectory_len = 10\n",
    "simulator = SinglePathSimulator(env, policy, n_trajectories, trajectory_len, **env_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    trpo_args = config['trpo_args']\n",
    "except:\n",
    "    trpo_args = {}\n",
    "\n",
    "trpo = TRPO(policy, value_fun, simulator, model_name=model_name,\n",
    "            continue_from_file=False, **trpo_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3335)\n",
      "tensor([[3625, 2289,  923,  ..., 3035, 1396, 1208],\n",
      "        [1920,  282, 1899,  ..., 3832, 2561,  222],\n",
      "        [3543, 1915, 3159,  ..., 2183,  954, 2857],\n",
      "        ...,\n",
      "        [ 340, 1922, 2323,  ..., 2181,  420, 3942],\n",
      "        [3949, 2238, 2692,  ..., 2958, 2359, 3327],\n",
      "        [ 561, 1305, 2313,  ...,  495,  215, 2353]], device='cuda:0')\n",
      "[EPISODE]: 1\t[AVG. REWARD]: 5.5228\t [ELAPSED TIME]: 0:00:10\n",
      "tensor(3390)\n",
      "tensor([[2426, 1973, 3543,  ..., 2709, 1182,  817],\n",
      "        [1237, 3776, 3158,  ..., 3879, 2570, 1545],\n",
      "        [2968, 3568, 1415,  ..., 2738, 3237, 2359],\n",
      "        ...,\n",
      "        [3909, 2717,  954,  ..., 3850,  305,  921],\n",
      "        [3676, 2205, 1361,  ...,  264, 3570,   16],\n",
      "        [2458,  214, 1875,  ..., 3568, 3326, 3341]], device='cuda:0')\n",
      "[EPISODE]: 2\t[AVG. REWARD]: 5.6139\t [ELAPSED TIME]: 0:00:22\n",
      "tensor(3411)\n",
      "tensor([[1209, 3919, 2274,  ..., 2855, 1027, 1392],\n",
      "        [1823, 1545, 1353,  ...,  362,  668, 1240],\n",
      "        [2021, 1492, 3850,  ..., 3076,  749, 3872],\n",
      "        ...,\n",
      "        [2715, 3543, 1242,  ..., 1634, 1923, 2647],\n",
      "        [3124, 1034, 2643,  ..., 3787, 1191, 1242],\n",
      "        [1859,  174, 2359,  ..., 1197, 3576, 2080]], device='cuda:0')\n",
      "[EPISODE]: 3\t[AVG. REWARD]: 5.6474\t [ELAPSED TIME]: 0:00:33\n",
      "tensor(3468)\n",
      "tensor([[  33, 1242, 2442,  ...,  903,   49, 3092],\n",
      "        [2383,  259,  669,  ...,  174, 3157, 2205],\n",
      "        [3909, 2945, 1135,  ..., 1584,  669, 1281],\n",
      "        ...,\n",
      "        [2799, 1353, 1033,  ...,  580, 2857, 2149],\n",
      "        [3909,  878, 1210,  ...,  104, 2862, 3567],\n",
      "        [1281, 3854, 2092,  ..., 3924, 3592,  214]], device='cuda:0')\n",
      "[EPISODE]: 4\t[AVG. REWARD]: 5.7429\t [ELAPSED TIME]: 0:00:45\n",
      "tensor(3501)\n",
      "tensor([[1420, 2231, 2931,  ..., 3912, 3915,  481],\n",
      "        [1825, 1160, 2158,  ..., 1545, 2605, 1353],\n",
      "        [2201, 1235, 2695,  ...,  105, 3116, 3568],\n",
      "        ...,\n",
      "        [1815, 2975, 3072,  ..., 1161, 1718, 1353],\n",
      "        [ 892, 2162,  448,  ..., 1899,  923,  669],\n",
      "        [2157, 1287, 3909,  ..., 2681, 1301,  592]], device='cuda:0')\n",
      "[EPISODE]: 5\t[AVG. REWARD]: 5.7977\t [ELAPSED TIME]: 0:00:57\n",
      "tensor(3564)\n",
      "tensor([[1242,  237, 3323,  ...,  906,  260, 2904],\n",
      "        [ 860, 3021, 1281,  ...,  287, 2237,  295],\n",
      "        [3854,  906, 3864,  ..., 1218, 1174, 1210],\n",
      "        ...,\n",
      "        [ 204, 2350, 3670,  ...,  865, 1883,   96],\n",
      "        [2425, 3744, 2595,  ..., 3909,  214, 3577],\n",
      "        [2958, 2763, 3469,  ...,  919, 1281, 3146]], device='cuda:0')\n",
      "[EPISODE]: 6\t[AVG. REWARD]: 5.9010\t [ELAPSED TIME]: 0:01:09\n",
      "tensor(3616)\n",
      "tensor([[3082, 3469, 2857,  ..., 1110, 1920, 2040],\n",
      "        [2082, 1922, 3445,  ..., 3181, 2019, 3851],\n",
      "        [3326,  937,  717,  ..., 3113, 1410, 3783],\n",
      "        ...,\n",
      "        [ 416, 3821, 2676,  ...,  295,   84, 2032],\n",
      "        [ 531, 1195,  448,  ..., 1287,  937, 2761],\n",
      "        [3264,  467, 2161,  ..., 1420,  105,  954]], device='cuda:0')\n",
      "[EPISODE]: 7\t[AVG. REWARD]: 5.9869\t [ELAPSED TIME]: 0:01:21\n",
      "tensor(3653)\n",
      "tensor([[1235,  895, 1274,  ..., 3274, 1623,  909],\n",
      "        [1553, 3919, 1305,  ..., 3534, 3852, 1353],\n",
      "        [1217,  954, 3455,  ..., 1135,  677, 2389],\n",
      "        ...,\n",
      "        [1375, 1545, 1281,  ..., 1205, 2627,  122],\n",
      "        [2717, 3053, 1162,  ...,  212, 2593, 2857],\n",
      "        [ 589, 2164, 2158,  ..., 3950,  531,  175]], device='cuda:0')\n",
      "[EPISODE]: 8\t[AVG. REWARD]: 6.0492\t [ELAPSED TIME]: 0:01:33\n",
      "tensor(3695)\n",
      "tensor([[ 155, 2999, 2494,  ...,  296, 1944, 2238],\n",
      "        [3657,  669, 2857,  ..., 1971,  308, 3851],\n",
      "        [ 662,  340, 3745,  ..., 2931,  351, 3568],\n",
      "        ...,\n",
      "        [1201, 3035, 2136,  ..., 1245, 2738,  923],\n",
      "        [3577, 3705, 2724,  ..., 1224,  106, 3085],\n",
      "        [ 387,  406,  857,  ..., 2995, 2919, 1569]], device='cuda:0')\n",
      "[EPISODE]: 9\t[AVG. REWARD]: 6.1184\t [ELAPSED TIME]: 0:01:44\n",
      "tensor(3739)\n",
      "tensor([[3850, 2255,  367,  ...,  945, 3949, 2493],\n",
      "        [  18, 1135, 2768,  ..., 1808,  265, 1899],\n",
      "        [1242, 3902, 2023,  ..., 3658,  222, 2699],\n",
      "        ...,\n",
      "        [1553,  122,  241,  ..., 3113, 2847,  363],\n",
      "        [1349, 1545, 1752,  ...,  306, 2323, 2904],\n",
      "        [2395, 3027, 3670,  ..., 1281,  918, 1098]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "n_episodes = 10\n",
    "trpo.train(n_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
